#Cache #CPU 

### Performance
>[!question] Execution time
>= CPU execution cycles + memory stall cycles
>= (실행 시간) = (CPU가 의미있게 작동하는 시간) + (메모리접근을 위해 멈춰있는 시간)

>[!question] CPU time
>$$
>=\ \#\ of\ insts\ \times\ CPI(\#\ of\ cycles\ per\ inst)\ \times\ T
>$$
>$$
>=\ clock\ cycles(총\ 소요되는\ 사이클\ 수)\ \times\ T
>$$
>$$
>=\ (CPU\ execution\ clock\ cycles\ +\ Memory\_stall\ clock\ cycles)\ \times\ T
>$$

>[!info] Assume
>cache hit does not require any extra CPU cycle for execution
>MA(memory access) stage takes 1 cycle upon a hit 
>(cache hit면 memory access 동작을 해당 cycle 안에 모두 완료 가능)

>[!question] Memory stall clock cycles
>$$
>= Stall\ Cycles_{read} + Stall\ Cycles_{write}
>$$
>$$
>= {memory\ accesses\over program} \ \times\ miss\ rate\ \times\ miss\ penalty
>$$
>$$
>= 총\ 메모리\ 접근대비\ miss\ 비율(프로그램의\ miss\ 개수)\ \times\ miss\ 1회당\ 패널티
>$$

### Impacts of Misses on Performance
#### Example 1
![](https://i.imgur.com/NXvGFNd.png)
#### Example 2
![](https://i.imgur.com/93znzeH.png)

### Average Memory Access Time (AMAT)

>[!question] 평균 메모리 접근 시간
>$$
>AMAT= Hit\ time + Miss\ rate\times Miss\ penalty
>$$

#### Example
![](https://i.imgur.com/5GiGGgo.png)

### Improving Cache Performance

>[!question] 단순히 캐시 사이즈를 키운다?
> miss rate⬇️, but Cache size가 너무 커지면 접근 시간도⬆️

$$
AMAT⬇️ = Hit\ time + Miss\ rate⬇️ \times Miss\ penalty⬇️
$$
> 1) reduces the miss rate
> 2) reduces the miss penalty

#### 1. Reducing Cache Miss Rates

- *Direct Mapped Cache*
	- : maps a memory block to exactly one location in cache
	- 메인 메모리의 특정한 한 block은 정확하게 cache의 한 block으로만 갈 수 있음
- *Fully Associative Cache*
	- : a memory block to be mapped to any cache block
	- 메인 메모리 block이 cache의 어디든지 갈 수 있음
	- only one set
- *N-Way Set Associative Cache*
	- : divides the cache into sets, each of which consists of n different locations (ways)
	- 메인 메모리 block이 unique한 set로만 갈 수 있음
	- set 1개 당 n개의 block을 가짐 (n개의 way 중에서는 어디든지 갈 수 있음)
	- A memory block is mapped to a unique set which is specified by the index field and can be placed in any way of that set (So memory block has n choices)

>[!question] Associative
> multi ways (to block) per set

![](https://i.imgur.com/IwfGhiP.png)
> *Direct Mapped* vs *2-way Set Associative* vs *Fully Associative*

![](https://i.imgur.com/e6Zx4OX.png)
> Associative Cache with 8 blocks

---
##### Ex) 4-way Set Associative Cache
![](https://i.imgur.com/LIqB6b2.png)
> Design of 4-way Set Associative Cache

- (block address) % (# of sets) = index of which set to search (LSB 10 bits)
- 4-ways (blocks) per set
	- compare tag (MSB 10 bits) among the blocks in the set to determine which block to search

![](https://i.imgur.com/XkqnjZO.png)
> *Fully Associative* vs *n-way Set Associative*

##### Ex) Cache with 4 blocks
1. 1 word per block
2. CPU generates 8 bits address to cache
![](https://i.imgur.com/X8nWFTz.png)
> Direct mapped cache
> 1. CPU generates 8 bits for cache —› block address is 6 bits except LSB 2 bits; byte offset
> 2. Cache has 4 sets with 4 blocks —› index is LSB 2 bits in block address

![](https://i.imgur.com/cJ4nLM1.png)
> 2-way Set Associative
> 1. CPU generates 8 bits for cache —› block address is 6 bits except LSB 2 bits; byte offset
> 2. Cache has 2-way set —› 2 set with 2 blocks per set —› index is LSB 1 bit in block address

![](https://i.imgur.com/ceSfXrK.png)
> Fully Associative (4-way Set Associative in this case)
> 1. CPU generates 8 bits for cache —› block address is 6 bits except LSB 2 bits; byte offset
> 2. Cache has 4-way set —› 1 set with 4 blocks per set —› no index
> —» miss rate⬇️, but time to search⬆️ and cost to make hardware⬆️ (many comparators)

##### Benefits of Set Associative Cache
![](https://i.imgur.com/NzKuwwS.png)
> 무조건 fully associative가 좋은 것은 아님. way가 증가할수록 하드웨어 cost 증가함

##### Range of Set Associative Caches
- **For a fixed size cache, the increase by a factor of 2 in associativity (associativity 2배)**
	- —› doubles the number of blocks per set (the number of ways) and halves the number of sets (way 2배, set 1/2배) 
	- ex) associativity 4 ways -> 8 ways : size of the index  -1 bit, size of the tag +1 bit

![](https://i.imgur.com/OpXaXuF.png)
>Block offset : block 안의 word 순서를 나타냄 
>Byte offset : word 안의 byte 순서를 나타냄

##### Replacement Policy
- **누구를 방출(evict)할 것인지에 대한 정책**
	- Least-recently used(LRU) 또는 Random 하게 방출

![](https://i.imgur.com/GCuIl4L.png)
>associativity가 작으면 LRU방식을, 크면 Random 방식을 주로 이용함.
>Valid bit 와 Dirty bit 는 way마다 필요하지만 LRU 는 set 마다 필요함.

#### 2. Reducing Cache Miss Penalty
**miss가 이미 발생했을 때, 패널티 줄이기 위해 cache를 multiple level로 나눔**

![](https://blog.kakaocdn.net/dn/cpaiv8/btrDKI9M6L7/wuZdNVOCRShzthpSqQhao0/img.png)
![](https://blog.kakaocdn.net/dn/bM42Et/btrDTQYKfku/DEtCYwQBtywvKn0fxCQom1/img.png)

- L1에서는 structure hazard (instruction fetch&memory access 동시에) 를 막기위해 나눴음.  
- L2와 L3는 instruction과 data를 위한 메모리를 따로 나누지않고 통합했음.  
	- instruction과 data의 비중이 동적으로 바뀔 수 있는 프로그램의 성격을 모른 채로 용량을 나누어버리면 용량 낭비가 심할 수 있음. 
	- 각 cache의 용량을 잘 쓰려면 unified가 좋음.

![](https://blog.kakaocdn.net/dn/dL6eiC/btrDRXcOZKX/6kZN7kfNslS6SbcVgdKHak/img.png)
>Core i7 Example

##### Multi Level Cache Performance Example
![](https://blog.kakaocdn.net/dn/cnWWAu/btrDQESMhMm/wkLrCnCcFKAXUSirTZf11k/img.png)
>L1만 있을 때보다 L2까지 있을 떄 CPI가 더 작음.

##### Multi Level Cache Design Considerations  
>[!question] Average Memory Access Time
>$$
>AMAT = Hit\ time + Miss\ rate \times Miss\ penalty
>$$

- L1 should focus on *minimizing hit time* in support of a *shorter clock cycle* 
	- : L1 is *smaller cache size* (i.e., faster) but have *higher miss rate*

- L2 (L3) should focus on *reducing miss rate* and *reducing L1 miss penalty*  
	- : *High associativity* and *large cache size* (conflict miss 줄이기 위해) 
	- For the L2 (L3) cache, hit time (better than main memory) is less important than miss rate
	- Miss penalty of L1 cache is significantly reduced by the presence of L2 and L3 caches

![](https://blog.kakaocdn.net/dn/B6IfP/btrDRXjBdQr/LfUMTgq1lMY0TIOY4kqfXk/img.png)
>Two Machines' Cache Parameters

---
##### Software Techniques  
**Try to exploit the cache structure in programming to avoid the cache misses and reduce the miss rate**
>[!info] Method
code optimizations (Loop interchange, Loop blocking)

##### Cache - friendly Code   
**row-major ordering 일때, 아래의 코드가 더 효율적임**

![](https://blog.kakaocdn.net/dn/eB0kys/btrDPtD3YyP/23UBB5XTTJNO6Nfg7KIKeK/img.png)
>Loop Interchange

*spatial* | 그 메모리 주변에 있는 것들이 앞으로 참조가 될 가능성이 높음
-|-
*temporal locality* | 그 메모리를 참조했을 당시 같이 참조했던 것들이 앞으로 참조가 될 가능성이 높음

**row-major와 col-major가 중요한 것은 ==spatial locality== 때문**
>[r&c - major order](https://huilife.tistory.com/entry/row-major-행-우선-column-major열-우선가-왜-중요한가)

###### Row - major 탐색
현재 Cache | 읽어야 하는 것 | hit?
--- | --- | ---
\-  | a11 | x
a11, a12, a13 | a12 | o
a11, a12, a13 | a13 | o
a11, a12, a13 | a21 | x
a21, a22, a23 | a22 | o
... | ... | ...
>9개의 원소를 읽는 동안 3번의 cache miss, 6번의 cache hit가 발생

###### Column - major 탐색
현재 Cache | 읽어야 하는 것 | hit?
-|-|-
\- | a11 | x
a11, a12, a13 | a21 | x
a21, a22, a23 | a31 | x
a31, a32, a33 | a12 | x
... | ... | ...
>9개의 원소를 읽는 동안 모두 cache miss 발생

