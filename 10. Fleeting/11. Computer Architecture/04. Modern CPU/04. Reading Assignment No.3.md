Focusing on improving the single thread performance by increasing ILP(out of order issue, multiple insts issue, aggressive branch prediction...) is diminishing, because of the limitation from the latency of memory and low ILP applications. So, Niagara processor is optimized for multithreaded performance so that increasing the throughput of machine. It increases TLP(thread level parallelism) and then is can hide the latency from memory.
Multithread processors requirs high bandwidth to memory system and it can support that using a crossbar interconnects scheme which routes to a banked on-chip L2 cache shared by all threads. The power consumption and dessipation of heat generated by server is important factor in operating data center. Improving performance in terms of power can not be achieved by single thread processor but multithreaded processor. Servers should support large throughput of client requests. Commercial server applications have poor ILP due to large working set and poor locality with memory access which results in cache misses frequently. Also difficulty with branch prediction and data dependency detection lead to task flush. Consequently, single thread processor is not desirable. On the other hand, server application have large TLP so that multiple coherent interconneted single threaded processors may have better perfomance. If aggregating muliple single cores on a single die sharing on-chip cache and high bandwidth off-chip memory, it will supports low latency when cores communicate each other.
Niagara contains 8 thread groups with 4 threads each(total 32 threads). Each thread group shares a processing pipeline; SPARC pipe which has L1 caches for insts and data. Zero cycle is needed to switch to another thread on the SPARC pipe so that memory and pipeline stalls can be hided. The 32 threads share L2 cache with 4-way banked and pipelined; 12-way set associative. The Niagara resolve the high latency due to off-chip access by coherence misses by providing  the communication link between SPARC pipes and L2 cache banks and other shred resources with crossbar interconnect. 
Each four threads of SPARC pipe has registers, instruction and store buffers respectively. The thread groups share L1 caches, TLB, execution units and most pipeline registers. Single issue pipeline with six stages. In the fetch stage, access to ITLB, inst cache and fetches two insts per cycle. A predecode bit in the cache informs long latency insts. In the thread select stage, thread select mux select a thread which can issue an inst. Thread select signal is common for thread select stage and fetch stage. When thread select stage choose the thread according to thread select signal, fetch stage choose same thread and access to inst cache to fetch an inst of that thread. The insts fetched from the cache and not available to be issued are stored to inst buffer. The insts of selected thread are decoded in decode stage and bypass unit bypass the data to dependent inst before register file update. The load store unit contains DTLB, data cache and store buffer. It is possible to check RAW hazard by checking the physical tags in the store buffer and store buffer bypasses the data to load inst. 
In multithreaded pipeline, there is structural hazard due to competition for shared hardware resources. The resources which have one inst per cycle throughput are no problem but like devider, which spend more than one cycle for one inst is problem. In that case, thread scheduler give priority to least recently executed thread so that let threads access to devider fairly. 
The thread scheduler predict that load insts are cache hits and then issue a dependent inst from the same thread, but this speculative thread has lower priority than non-speculative thread. 
The integer register file has three read and two write ports. The register implements with window of eight ins, locals, outs. The out registers are address of "in" registers of sequential window and have same physical registers. New window is requested when procedure calls. Here, the window slides up and the old outputs become new inputs. When returning from call, the window slide down again.
The L1 inst cache is 4 way set associative. Fetch stage fetches two insts per cycle from inst cache. L1 data cache is also 4 way set associative, and implements write through. The server applications have large work set and then large L1 cache is needed to reduce the cache miss rates. However the four threads in each thread group can hide the latency from L1 and L2 cache misses. Niagara implements L1 caches write through with allocate on load and no allocate on store. If load miss in L1 cache, then that is delivered to the source bank of L2 cache and the missed load address is recorded to tag of L1 cache. The missing line is delivered to L1 cache from L2 cache. In case of store, update the local cache after updating the L2 cache. The crossbar sets the order of memory between each other banks of L2 caches and delivers the transactions to L1 cache in the same order.
Niagara make the multithreaded CPU for Solaris OS and the Solaris applications can be run without any modification.