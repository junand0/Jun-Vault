## Improving the performance of single program or single thread

### Instruction Issue
![400](https://i.imgur.com/bms6qPU.png)
>1 instruction width pipeline
 $\to$ decrease function unit utilization due to dependencies

### Superscalar Issue
![400](https://i.imgur.com/orLhaZu.png)
> superscalar leads to more performance, but low utilization

### Very Long Instruction Words : VLIW
![600](https://i.imgur.com/KGxu80w.png)
>make group with independent instructions $\to$ VLIW

### Predicated Issue (usually with VLIW)
![400](https://i.imgur.com/RCcKpQA.png)
>increases function unit utilization, but results are thrown away (flushed)

## Hardware Multithreading
### Performance and Utilization
- Performance (IPC) is important
	- Utilization (actual IPC / peak IPC) is important too
- Even moderate superscalars (4-way) not fully utilized
	- Average sustained IPC is 1.5~2 < 50% utilization
		- Branch mis-predictions
		- Cache misses, especially L2
		- Data dependency
- Multi-threading (MT)
	- One thread cannot fully utilize CPU
	- Improve utilization by executing multiple threads simultaneously on a single CPU
### Latency vs Throughput
- MT trades latency for throughput
	- - Sharing processor increases latency of *individual threads*
	- + But improves *aggregate latency* of both threads
	- + Improves utilization
- Example :
	- Thread A : individual latency = 10s, latency with thread B = 15s
	- Thread B : individual latency = 20s, latency with thread A = 25s
		- Sequential latency = 30s
		- Parallel latency = 25s
	- MT slows each thread my 5s
	- But improves total latency by 5s
- Different workloads have different parallelism
	- SpecFP has lot of ILP $\to$ superscalar
	- Server workloads have TLP $\to$ multi thread
		- TLP : thread level parallelism
### MT : Core Sharing
- *Time sharing* :
	- Run one thread at a time
	- On a long latency operation such as cache miss, switch threads
		- context switching
	- Also know as switch-on-miss multithreading
- *Space sharing*
	- Simple : across pipeline depth (pipeline interleaving)
		- Fetch and issue each cycle from a different thread
	- Advanced : both across pipeline depth and width
		- Fetch and issue each cycle from multiple threads
		- Policy to decide which to fetch become complicated
		- Simultaneous Multithreading (SMT)
### Multi Core : Chip-Multiprocessor (CMP)
![400](https://i.imgur.com/4MKtkA1.png)
>한 chip 안에 multiple cores
![400](https://i.imgur.com/DUxE3dv.png)
>Limited utilization when only running one thread

#### CMP
- Putting multiple smaller and simpler cores on chip
- Easy to build by filling the area with multiple simple cores
- need enough memory and I/O bandwidth
- Applications must have enough TLP
	- App must be divided to a group of threads
	- Threads must run in parallel
	- server, matrix computation...

### ILP vs TLP
- Granularities of parallelism
	- Instruction / basic block ; small ILP
	- Loop ; large ILP
	- Tasks ; threads
	- Processes ; programs
![600](https://i.imgur.com/JlQuctP.png)

### Superscalar vs CMP
![](https://i.imgur.com/QXP3Ubn.png)

![](https://i.imgur.com/y6syupV.png)
>Area is price

### Niagara : CMP with multi-threading
![400](https://i.imgur.com/EQx3LP3.png)
>8 cores with 4-way multi-thread
![](https://i.imgur.com/exjV8Q5.png)
- Each core pipeline with *4-way in-order multi-threading*
- time sharing : core 1개당 외부에서 core 4개로 보임 (4-way multi-threading)
- 6 shallow stages
- No branch prediction
- Select lest recently used thread for fairness
- Minimal speculation ; only load hit speculation
### Coarse-grain Multithreading
![500](https://i.imgur.com/mqRdtCv.png)
- long latency operation가 발생했을 때만 switching을 해준다
- Thus preserves single thread performance
- but can only hide very long latencies (L2, L3 miss)
- *Coarse Grain Multi-Threading : CGMT*
	- + Sacrifices very little single thread performance
	- - Tolerates only long latencies
	- Thread scheduling policy
		- Designate a preferred (prioritized) thread
			- Switch to thread B on thread A's L2 miss
			- Switch back to A when A's L2 miss resolved
	- No pipeline partitioning
		- They flush slow thread on switch
			- Can't tolerate latencies shorter than twice pipeline depth
				- switch penalty = \# flushed cycle = twice pipeline depth
				- \# latency cycle > switch penalty $\to$ switch
			- Need short in-order pipeline for good performance
![400](https://i.imgur.com/AgKT1Au.png)

### Fine Grained Multi-threading
![400](https://i.imgur.com/1u9ewuK.png)
- Inter thread dependencies still limit performance
	- $\to$ switch threads very fast and frequently
- - Sacrifices significant single thread performance
- + Tolerate all latencies, not only short latency
- Thread scheduling policy
	- Switch threads every cycle (round-robin), L2 miss or no
- Pipeline partitioning
	- Dynamically partitioned, no flush on switch
	- Length of pipeline doesn't matter
- Need a lot of threads
- Not popular today 
	- Many threads $\to$ many register files
![500](https://i.imgur.com/4xkb9ZQ.png)
>Many threads
>Multiple threads in pipeline at once

### Simultaneous Multithreading : SMT
![400](https://i.imgur.com/scHeErD.png)
>Space Sharing

- Renaming table 4개 필요
- Physical register file이 충분히 커야함 
	- share physical register file (개수가 4개 필요한 것은 아님)
- Single thread의 performance를 줄이는 FGMT 단점 해결
- Maximize utilization of function units by independent operations
- Hyperthreading

- Multi-threading on out-of-order machine
	- performance benefits
	- natural tolerance of Dcache miss latency
		- working set > cache size $\to$ degrade performance
		- working set < cache size인 operation 조합을 찾아서 issue하는 것이 좋음
- SMT
	- + Tolerates all latencies
	- $\pm$ Sacrifices some single thread performance due to resource sharing
	- Thread scheduling policy
		- Round-robin
	- Pipeline partitioning
		- Dynamic, but hard to be optimal
![500](https://i.imgur.com/DwPyWPD.png)

#### Issues for SMT
- Fetch
	- like Niagara, can switch multiple PCs every cycle or
	- few independent fetch
- Cache interference
	- General concern for all MT variants
	- Can the working sets of multiple threads fit in the caches?
	- Memory sharing thread applications can help here
		- + Same instructions $\to$ share Icache
		- + Shared data $\to$ less Dcache contention
		- MT is good for server workloads
	- To keep miss rates low, SMT might need a larger L2 cache
		- Out-of-order tolerates L1 misses
- Large map table and physical register file
	- \# physical registers = \# threads x \# architectural registers + \# in-flight insts
	- renaming을 위해 실제로 더 커야함
#### SMT Resource Partitioning
- How are ROB/LSQ/ISQ partitioned in SMT?
- *Static partitioning*
	- Divide ROB/LSQ/ISQ into T static equal-sized partitions
	- +Ensures that low-IPC threads don't starve high-IPC ones
		- Low-IPC threads stall and occupy ROB/LSQ/ISQ slots
	- Low utilization
		- ISQ의 크기를 제한하면 그 제한 내에서 out-of-order issue 조합이 나오기 때문에 ILP를 잘 활용하지 못할 수 있다.
- *Dynamic partitioning*
	- Divide ROB/LSQ/ISQ into dynamically resizing partitions
	- Let threads fight for among themselves
	- + High utilization
	- - Possible starvation
	- ex) ICOUNT : fetch a thread having fewest in-flight insts

#### SMT vs CMP
- CMP
	- multiple separate pipelines
	- simpler, possibly faster clk
- SMT
	- a single larger pipeline
	- better IPC on a single thread
- use both SMT and CPM
	- 8 OoO cores with 2-way SMT, each with 8 FGMT threads
	- : 8 cores x 8 threads = 64 threads on-chip with 16 thread issue a cycle
![](https://i.imgur.com/GBvr3se.png)

![](https://i.imgur.com/5PFTrmC.png)
